---
title: "Contrastively Learning Visual Attention as Affordance Cues from Demonstrations for Robotic Grasping"
collection: publications
permalink: /publication/2021-12-16-Contrastively-Learning-Visual-Attention-as-Affordance-Cues-from-Demonstrations-for-Robotic-Grasping
excerpt: 'Conventional works that learn grasping affordance from demonstrations need to explicitly predict grasping configurations, such as gripper approaching angles or grasping preshapes. Classic motion planners could then sample trajectories by using such predicted configurations. In this work, our goal is instead to fill the gap between affordance discovery and affordance-based policy learning by integrating the two objectives in an end-to-end imitation learning framework based on deep neural networks. From a psychological perspective, there is a close association between attention and affordance. Therefore, with an end-to-end neural network, we propose to learn affordance cues as visual attention that serves as a useful indicating signal of how a demonstrator accomplishes tasks, instead of explicitly modeling affordances. To achieve this, we propose a contrastive learning framework that consists of a Siamese encoder and a trajectory decoder. We further introduce a coupled triplet loss to encourage the discovered affordance cues to be more affordance-relevant. Our experimental results demonstrate that our model with the coupled triplet loss achieves the highest grasping success rate in a simulated robot environment. Our project website can be accessed through the project website (please refer to the paper for the link).'
date: 2021-12-16
venue: '2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)'
paperurl: 'https://ieeexplore.ieee.org/document/9636760'
citation: 'Y. Zha, S. Bhambri and L. Guan, "Contrastively Learning Visual Attention as Affordance Cues from Demonstrations for Robotic Grasping," 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021, pp. 7835-7842, doi: 10.1109/IROS51168.2021.9636760.'
---
**Abstract**: Conventional works that learn grasping affordance from demonstrations need to explicitly predict grasping configurations, such as gripper approaching angles or grasping preshapes. Classic motion planners could then sample trajectories by using such predicted configurations. In this work, our goal is instead to fill the gap between affordance discovery and affordance-based policy learning by integrating the two objectives in an end-to-end imitation learning framework based on deep neural networks. From a psychological perspective, there is a close association between attention and affordance. Therefore, with an end-to-end neural network, we propose to learn affordance cues as visual attention that serves as a useful indicating signal of how a demonstrator accomplishes tasks, instead of explicitly modeling affordances. To achieve this, we propose a contrastive learning framework that consists of a Siamese encoder and a trajectory decoder. We further introduce a coupled triplet loss to encourage the discovered affordance cues to be more affordance-relevant. Our experimental results demonstrate that our model with the coupled triplet loss achieves the highest grasping success rate in a simulated robot environment. Our project website can be accessed [here](https://sites.google.com/asu.edu/affordance-aware-imitation/project).

[Download paper here](https://github.com/sbhambr1/siddhantbhambri.github.io/raw/master/files/Contrastively%20Learning%20Visual%20Attention%20as%20Affordance%20Cues%20from%20Demonstrations%20for%20Robotic%20Grasping.pdf)

<!-- Recommended citation: Y. Zha, S. Bhambri and L. Guan, "Contrastively Learning Visual Attention as Affordance Cues from Demonstrations for Robotic Grasping," 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021, pp. 7835-7842, doi: 10.1109/IROS51168.2021.9636760. -->